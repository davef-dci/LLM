## Using RAG with a local LLM hosted by Ollama

https://dev.to/nassermaronie/build-your-own-rag-app-a-step-by-step-guide-to-setup-llm-locally-using-ollama-python-and-chromadb-b12
    Code at https://github.com/firstpersoncode/local-rag

https://medium.com/rahasak/build-rag-application-using-a-llm-running-on-local-computer-with-ollama-and-langchain-e6513853fda0
    Runs locally using Ollama, using Flask to create a REST API,
    Ollama running in a Docker container, and an instance of
    MongoDB running somewhere (but it's never actually
    referenced).

https://www.youtube.com/watch?v=ztBJqzBU5kc
    Uses something called Streamlit, which doesn't work well
    locally.

https://www.youtube.com/watch?v=2TJxpyO3ei4&list=WL&index=7
    Don't use, the code is haphazard and does not run
    out-of-the-box.

Use Ollama to host the model

## References

https://thenewstack.io/how-to-get-started-running-small-language-models-at-the-edge/

https://www.arsturn.com/blog/running-ollama-on-nvidia-gpus-a-comprehensive-guide

1. install Ollama
1. https://github.com/ollama/ollama/tree/main/docs

```
curl -fsSL https://ollama.com/install.sh | sh
```

1. pull two models: mistral, nomic-embed-text
1. start "ollama serve"

# to embed a file
curl --request POST --url http://localhost:8080/embed --header 'Content-Type:
multipart/form-data' --form file='@/home/bill/Downloads/XPRIZE Rainforest Rules &
Regulations_v1.0.pdf'

# to submit a query
curl --request POST --url http://localhost:8080/query --header 'Content-Type:
application/json' --data '{ "query": "What is the XPRIZE?" }'
